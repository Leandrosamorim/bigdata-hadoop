{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure to link pyspark to the right Spark folder with findspark\n",
    "import findspark\n",
    "from functools import wraps\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"data-formats\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:54:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `20230515_000000.jsonl': File exists\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -put ../datasets/20230515_000000.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:54:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 7 items\n",
      "drwxr-xr-x   - root supergroup          0 2023-08-10 00:54 /user/root/.sparkStaging\n",
      "-rw-r--r--   2 root supergroup     46.8 M 2023-08-09 23:48 /user/root/20230515_000000.jsonl\n",
      "drwxr-xr-x   - root supergroup          0 2023-08-09 23:53 /user/root/20230515_000000.parquet\n",
      "drwxr-xr-x   - root supergroup          0 2023-08-09 23:45 /user/root/bus-api-small\n",
      "drwxr-xr-x   - root supergroup          0 2023-08-10 00:39 /user/root/csv\n",
      "drwxr-xr-x   - root supergroup          0 2023-08-10 00:43 /user/root/orc\n",
      "drwxr-xr-x   - root supergroup          0 2023-08-10 00:38 /user/root/parquet\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /user/root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversão pré-carregamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datahora         =>   integer\n",
    "# datahoraenvio    =>   integer\n",
    "# datahoraservidor =>   integer\n",
    "# latitude         =>   float/double/numeric\n",
    "# longitude        =>   float/double/numeric\n",
    "# linha            =>   string\n",
    "# ordem            =>   string\n",
    "# velocidade       =>   integer\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"datahora\", IntegerType(), True),\n",
    "    StructField(\"datahoraenvio\", IntegerType(), True),\n",
    "    StructField(\"datahoraservidor\", IntegerType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"linha\", StringType(), True),\n",
    "    StructField(\"ordem\", StringType(), True),\n",
    "    StructField(\"velocidade\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 ms, sys: 1.59 ms, total: 3.67 ms\n",
      "Wall time: 899 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.schema(schema).json(\"/user/root/20230515_000000.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datahora: integer (nullable = true)\n",
      " |-- datahoraenvio: integer (nullable = true)\n",
      " |-- datahoraservidor: integer (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- linha: string (nullable = true)\n",
      " |-- ordem: string (nullable = true)\n",
      " |-- velocidade: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(datahora=None, datahoraenvio=None, datahoraservidor=None, latitude=None, longitude=None, linha=None, ordem=None, velocidade=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não funciona o carregamento passando o schema no caso de jsonl!<br/>\n",
    "Valores são carregados como `None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversão pós-carregamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.72 ms, sys: 855 µs, total: 2.58 ms\n",
      "Wall time: 764 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.json(\"/user/root/20230515_000000.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datahora: string (nullable = true)\n",
      " |-- datahoraenvio: string (nullable = true)\n",
      " |-- datahoraservidor: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- linha: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- ordem: string (nullable = true)\n",
      " |-- velocidade: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(datahora='1684119592000', datahoraenvio='1684119600000', datahoraservidor='1684119620000', latitude='-22,87652', linha='LECD36', longitude='-43,36818', ordem='C51641', velocidade='37')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valores todos carregados como `string`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a conversão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"datahora\", col(\"datahora\").cast(\"long\")\n",
    ").withColumn(\n",
    "    \"datahoraenvio\", col(\"datahoraenvio\").cast(\"long\")\n",
    ").withColumn(\n",
    "    \"datahoraservidor\", col(\"datahoraservidor\").cast(\"long\")\n",
    ").withColumn(\n",
    "    \"latitude\", regexp_replace(col(\"latitude\"), \",\", \".\").cast(\"double\")\n",
    ").withColumn(\n",
    "    \"longitude\", regexp_replace(col(\"longitude\"), \",\", \".\").cast(\"double\")\n",
    ").withColumn(\n",
    "    \"velocidade\", col(\"velocidade\").cast(\"integer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datahora: long (nullable = true)\n",
      " |-- datahoraenvio: long (nullable = true)\n",
      " |-- datahoraservidor: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- linha: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- ordem: string (nullable = true)\n",
      " |-- velocidade: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(datahora=1684119592000, datahoraenvio=1684119600000, datahoraservidor=1684119620000, latitude=-22.87652, linha='LECD36', longitude=-43.36818, ordem='C51641', velocidade=37)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escrevendo em apenas um arquivo Parquet<br/>\n",
    "Lembrando que devemos passar um diretório como Path para escrita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:54:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/10 00:54:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/parquet\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/root/parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.86 ms, sys: 1.98 ms, total: 3.84 ms\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.coalesce(1).write.parquet(\"/user/root/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:54:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   2 root supergroup          0 2023-08-10 00:54 /user/root/parquet/_SUCCESS\n",
      "-rw-r--r--   2 root supergroup      2.2 M 2023-08-10 00:54 /user/root/parquet/part-00000-affdba43-db1e-49a5-ab5f-40c1074d870b-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /user/root/parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46.8 MB vs 2.2 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 ms, sys: 1.42 ms, total: 4.23 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfp = spark.read.parquet(\"/user/root/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datahora: long (nullable = true)\n",
      " |-- datahoraenvio: long (nullable = true)\n",
      " |-- datahoraservidor: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- linha: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- ordem: string (nullable = true)\n",
      " |-- velocidade: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportando de Parquet para demais tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:54:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/10 00:54:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/root/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.32 ms, sys: 1.65 ms, total: 5.97 ms\n",
      "Wall time: 678 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfp.coalesce(1).write.option(\"header\", True).csv(\"/user/root/csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:54:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   2 root supergroup          0 2023-08-10 00:54 /user/root/csv/_SUCCESS\n",
      "-rw-r--r--   2 root supergroup     17.2 M 2023-08-10 00:54 /user/root/csv/part-00000-778e7551-736b-4ae7-b961-e2dcebc59372-c000.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /user/root/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.35 ms, sys: 1.15 ms, total: 4.49 ms\n",
      "Wall time: 315 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.option(\"header\", True).csv(\"/user/root/csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datahora: string (nullable = true)\n",
      " |-- datahoraenvio: string (nullable = true)\n",
      " |-- datahoraservidor: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- linha: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- ordem: string (nullable = true)\n",
      " |-- velocidade: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:54:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ls: `/user/root/avro': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /user/root/avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriter' object has no attribute 'avro'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5629927ff07b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/user/root/avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'avro'"
     ]
    }
   ],
   "source": [
    "dfp.coalesce(1).write.avro(\"/user/root/avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar dependência da biblioteca `Spark Avro`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:55:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/10 00:55:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/orc\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/root/orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.63 ms, sys: 2.11 ms, total: 5.74 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfp.coalesce(1).write.orc(\"/user/root/orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:55:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   2 root supergroup          0 2023-08-10 00:55 /user/root/orc/_SUCCESS\n",
      "-rw-r--r--   2 root supergroup      3.1 M 2023-08-10 00:55 /user/root/orc/part-00000-b757656e-3dfb-46a5-a766-4a2affbe4019-c000.snappy.orc\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /user/root/orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.03 ms, sys: 3.62 ms, total: 8.65 ms\n",
      "Wall time: 48.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.orc(\"/user/root/orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datahora: long (nullable = true)\n",
      " |-- datahoraenvio: long (nullable = true)\n",
      " |-- datahoraservidor: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- linha: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- ordem: string (nullable = true)\n",
      " |-- velocidade: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo para debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/10 00:56:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "datahora,datahoraenvio,datahoraservidor,latitude,linha,longitude,ordem,velocidade\n",
      "1684119592000,1684119600000,1684119620000,-22.87652,LECD36,-43.36818,C51641,37\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/csv/*.csv | head -n 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
