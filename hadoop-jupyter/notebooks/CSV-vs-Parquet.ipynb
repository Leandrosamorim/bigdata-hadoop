{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV-vs-Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure to link pyspark to the right Spark folder with findspark\n",
    "import findspark\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import wraps\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"csv-vs-parquet\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/03 00:01:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -put ../datasets/f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_LAP_TIMES_PATH = \"hdfs://node-master:9000/user/root/f1/lapTimes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_LAP_TIMES_DEST_PATH = \"hdfs://node-master:9000/user/root/f1/lapTimes.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_LAP_TIMES_DEST_PATH_2 = \"hdfs://node-master:9000/user/root/f1/lapTimes2.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/03 00:05:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 13 items\n",
      "-rw-r--r--   2 root supergroup       8667 2023-08-03 00:01 /user/root/f1/circuits.csv\n",
      "-rw-r--r--   2 root supergroup     224140 2023-08-03 00:01 /user/root/f1/constructorResults.csv\n",
      "-rw-r--r--   2 root supergroup     267256 2023-08-03 00:01 /user/root/f1/constructorStandings.csv\n",
      "-rw-r--r--   2 root supergroup      15617 2023-08-03 00:01 /user/root/f1/constructors.csv\n",
      "-rw-r--r--   2 root supergroup     768136 2023-08-03 00:01 /user/root/f1/driverStandings.csv\n",
      "-rw-r--r--   2 root supergroup      79533 2023-08-03 00:01 /user/root/f1/drivers.csv\n",
      "-rw-r--r--   2 root supergroup   12118621 2023-08-03 00:01 /user/root/f1/lapTimes.csv\n",
      "-rw-r--r--   2 root supergroup     220898 2023-08-03 00:01 /user/root/f1/pitStops.csv\n",
      "-rw-r--r--   2 root supergroup     315477 2023-08-03 00:01 /user/root/f1/qualifying.csv\n",
      "-rw-r--r--   2 root supergroup     104839 2023-08-03 00:01 /user/root/f1/races.csv\n",
      "-rw-r--r--   2 root supergroup    1176858 2023-08-03 00:01 /user/root/f1/results.csv\n",
      "-rw-r--r--   2 root supergroup       4099 2023-08-03 00:01 /user/root/f1/seasons.csv\n",
      "-rw-r--r--   2 root supergroup       1926 2023-08-03 00:01 /user/root/f1/status.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root/f1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/03 00:07:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.6 M  /user/root/f1/lapTimes.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -h /user/root/f1/lapTimes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-6.0.1-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (23.9 MB)\n",
      "     |################################| 23.9 MB 195 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.19.5)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-6.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(\"../datasets/f1/lapTimes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_folder = Path(\"../datasets/f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"raceId\",\n",
    "    \"driverId\",\n",
    "    \"lap\",\n",
    "    \"position\",\n",
    "    \"time\",\n",
    "    \"milliseconds\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas==1.1.5\r\n"
     ]
    }
   ],
   "source": [
    "! pip freeze | grep pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath, dtype={field: str for field in fields})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raceId          object\n",
       "driverId        object\n",
       "lap             object\n",
       "position        object\n",
       "time            object\n",
       "milliseconds    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raceId               841\n",
       "driverId              20\n",
       "lap                    1\n",
       "position               1\n",
       "time            1:38.109\n",
       "milliseconds       98109\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"raceId\"] = df[\"raceId\"].str.replace(r'\\D+', '', regex=True).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"driverId\"] = df[\"driverId\"].str.replace(r'\\D+', '', regex=True).astype('int')\n",
    "df[\"lap\"] = df[\"lap\"].str.replace(r'\\D+', '', regex=True).astype('int')\n",
    "df[\"position\"] = df[\"position\"].str.replace(r'\\D+', '', regex=True).astype('int')\n",
    "df[\"milliseconds\"] = df[\"milliseconds\"].str.replace(r'\\D+', '', regex=True).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raceId           int64\n",
       "driverId         int64\n",
       "lap              int64\n",
       "position         int64\n",
       "time            object\n",
       "milliseconds     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(dest_folder / \"lapTimes.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflt = spark.read.format(\"csv\").option(\"header\", \"true\").load(F1_LAP_TIMES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dflt.head(1)[0][\"raceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"raceId\", IntegerType(), True),\n",
    "    StructField(\"driverId\", IntegerType(), True),\n",
    "    StructField(\"lap\", IntegerType(), True),\n",
    "    StructField(\"position\", IntegerType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"milliseconds\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflt2 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(F1_LAP_TIMES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(raceId=841, driverId=20, lap=1, position=1, time='1:38.109', milliseconds=98109)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflt2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflt2.write.parquet(F1_LAP_TIMES_DEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/03 00:38:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0      /user/root/f1/lapTimes.parquet/_SUCCESS\n",
      "1.8 M  /user/root/f1/lapTimes.parquet/part-00000-6f9a116b-98ea-45a2-a81b-092a5b6cc4e7-c000.snappy.parquet\n",
      "1.1 M  /user/root/f1/lapTimes.parquet/part-00001-6f9a116b-98ea-45a2-a81b-092a5b6cc4e7-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -h /user/root/f1/lapTimes.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflt2.coalesce(1).write.parquet(F1_LAP_TIMES_DEST_PATH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/03 00:41:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0      /user/root/f1/lapTimes2.parquet/_SUCCESS\n",
      "2.5 M  /user/root/f1/lapTimes2.parquet/part-00000-7bdf9fdc-635c-4ee2-943f-f54e15faf910-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -h /user/root/f1/lapTimes2.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----+\n",
      "|driverId|         driverRef|races|\n",
      "+--------+------------------+-----+\n",
      "|      22|       barrichello|  326|\n",
      "|      18|            button|  309|\n",
      "|      30|michael_schumacher|  308|\n",
      "|       4|            alonso|  293|\n",
      "|       8|         raikkonen|  273|\n",
      "|      13|             massa|  271|\n",
      "|     119|           patrese|  257|\n",
      "|      15|            trulli|  256|\n",
      "|      14|         coulthard|  247|\n",
      "|      21|        fisichella|  231|\n",
      "+--------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfr.join(\n",
    "    dfd, dfr.driverId == dfd.driverId, \"inner\"\n",
    ").groupBy(\n",
    "    dfr.driverId, dfd.driverRef\n",
    ").agg(\n",
    "    count(dfr.raceId).alias(\"races\")\n",
    ").orderBy(\n",
    "    col(\"races\").desc()\n",
    ").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr.registerTempTable(\"races\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.registerTempTable(\"drivers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----+\n",
      "|driverId|         driverRef|races|\n",
      "+--------+------------------+-----+\n",
      "|      22|       barrichello|  326|\n",
      "|      18|            button|  309|\n",
      "|      30|michael_schumacher|  308|\n",
      "|       4|            alonso|  293|\n",
      "|       8|         raikkonen|  273|\n",
      "|      13|             massa|  271|\n",
      "|     119|           patrese|  257|\n",
      "|      15|            trulli|  256|\n",
      "|      14|         coulthard|  247|\n",
      "|      21|        fisichella|  231|\n",
      "+--------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select r.driverId, d.driverRef, count(0) races\n",
    "from races r\n",
    "  inner join drivers d on r.driverId = d.driverId\n",
    "group by r.driverId, d.driverRef\n",
    "order by races desc\n",
    "limit 10\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
